#Default params for running indexing using Pignlproc scripts
# input: (uncompressed??) Wikipedia XML dump
#INPUT=/home/chris/projects/pignlproc/src/test/resources/enwiki-latest-pages-articles-1000.xml
INPUT=/user/hadoop/wikidump/20120601/enwiki-20120601-pages-articles.xml

#Configuration
MAX_SPAN_LENGTH=5000
MIN_COUNT=5
MIN_CONTEXTS=5
N=150
#this constant is chosen based upon the reported number of DBpediaResources
NUM_DOCS=3000000

#Lucene Analyzer to use (language-specific)
LANG=en
ANALYZER_NAME=EnglishAnalyzer
#LANG=fr
#ANALYZER_NAME=FrenchAnalyzer

# output directory in HDFS
OUTPUT_DIR=/user/hadoop/output/top150-filter5-20120601

#Location of the Uri list for filtering
URI_LIST=/user/hadoop/input/milne_uri_list.txt

# location of the stop word list file in HDFS
STOPLIST_NAME=stopwords.en.list
STOPLIST_PATH=/user/hadoop/input
# local path to JAR containing UDFs
PIGNLPROC_JAR=/local/pignlproc/pignlproc-0.1.0-SNAPSHOT.jar
# number of reducers
DEFAULT_PARALLEL=12

# compression of output dataset
#COMPRESS_OUTPUT=
#OUTPUT_COMPRESSION_CODEC=org.apache.hadoop.io.compress.GzipCodec
